{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AI - TP1_2\n",
    "\n",
    "Bastien SAUVAT et Bastien FAISANT\n",
    "\n",
    "# Exercise 3 : Text classification on the Ohsumed dataset\n",
    "\n",
    "*Objective : The goal of this exercise is to realize a text classifier using deep neural networks. Your task\n",
    "is to construct a classifier, using the available training set, and evaluate it using the test set. The classifier\n",
    "should predict the category for the articles.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import defaultdict\n",
    "from keras.preprocessing.text import text_to_word_sequence, Tokenizer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from functools import reduce\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer  # to encode text to int\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences   # to do padding or truncating\n",
    "from tensorflow.keras.models import Sequential     # the model\n",
    "from tensorflow.keras.layers import Embedding, Dropout, GlobalAveragePooling1D, Bidirectional, LSTM, Dense, MaxPooling1D, Conv1D # layers of the architecture\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint   # save model\n",
    "from tensorflow.keras.models import load_model   # load saved model\n",
    "from tensorflow.keras import regularizers\n",
    "import re\n",
    "import string\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data parsing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by parsing the Ohsumed dataset in order to create two dataframes (training and test) with one column for the texts and another for the associated classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_info(path: str):\n",
    "    data = list(os.walk(path))[1:]\n",
    "    files = []\n",
    "    for d in data:\n",
    "        folder_name = d[0]\n",
    "        for file in d[2]:\n",
    "            files.append((folder_name.split('/')[-1], os.path.join(folder_name, file)))\n",
    "\n",
    "    d = defaultdict(int)\n",
    "    texts = defaultdict(list)\n",
    "    for (cate, file) in files:\n",
    "        with open(file, 'r') as outfile:\n",
    "            text = outfile.read()\n",
    "            texts[cate].append(text)\n",
    "            words = text_to_word_sequence(text)\n",
    "            for word in words:\n",
    "                d[word] += 1\n",
    "    words = sorted(d.items(), key=lambda x: x[1], reverse=True)\n",
    "    return (texts, words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_texts, training_words = get_info(\"./data/ohsumed-first-20000-docs/training/\")\n",
    "test_texts, test_words = get_info(\"./data/ohsumed-first-20000-docs/test/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df(dataset: defaultdict[any, list]):\n",
    "    classes = []\n",
    "    texts = []\n",
    "    for classe, liste_texts in dataset.items():\n",
    "        for text in liste_texts:\n",
    "            texts.append(text)\n",
    "            classes.append(classe)\n",
    "\n",
    "    df = pd.DataFrame({'Classes': classes, 'Texts': texts})\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = get_df(training_texts)\n",
    "test_set = get_df(test_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Classes</th>\n",
       "      <th>Texts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C01</td>\n",
       "      <td>Augmentation mentoplasty using Mersilene mesh....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C01</td>\n",
       "      <td>Multiple intracranial mucoceles associated wit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C01</td>\n",
       "      <td>Replacement of an aortic valve cusp after neon...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C01</td>\n",
       "      <td>The value of indium 111 leukocyte scanning in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C01</td>\n",
       "      <td>Febrile infants less than eight weeks old. Pre...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Classes                                              Texts\n",
       "0     C01  Augmentation mentoplasty using Mersilene mesh....\n",
       "1     C01  Multiple intracranial mucoceles associated wit...\n",
       "2     C01  Replacement of an aortic valve cusp after neon...\n",
       "3     C01  The value of indium 111 leukocyte scanning in ...\n",
       "4     C01  Febrile infants less than eight weeks old. Pre..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10433 entries, 0 to 10432\n",
      "Data columns (total 2 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   Classes  10433 non-null  object\n",
      " 1   Texts    10433 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 163.1+ KB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Classes</th>\n",
       "      <th>Texts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10433</td>\n",
       "      <td>10433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>23</td>\n",
       "      <td>6286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>C23</td>\n",
       "      <td>Magnetic resonance imaging of radiation optic ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1799</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Classes                                              Texts\n",
       "count    10433                                              10433\n",
       "unique      23                                               6286\n",
       "top        C23  Magnetic resonance imaging of radiation optic ...\n",
       "freq      1799                                                  6"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set.info()\n",
    "train_set.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 12733 entries, 0 to 12732\n",
      "Data columns (total 2 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   Classes  12733 non-null  object\n",
      " 1   Texts    12733 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 199.1+ KB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Classes</th>\n",
       "      <th>Texts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>12733</td>\n",
       "      <td>12733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>23</td>\n",
       "      <td>7643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>C23</td>\n",
       "      <td>The butterfly rash and the malar flush. What d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>2153</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Classes                                              Texts\n",
       "count    12733                                              12733\n",
       "unique      23                                               7643\n",
       "top        C23  The butterfly rash and the malar flush. What d...\n",
       "freq      2153                                                  7"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set.info()\n",
    "test_set.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set - Examples per Category:\n",
      "Classes\n",
      "C23    1799\n",
      "C14    1249\n",
      "C04    1163\n",
      "C10     621\n",
      "C06     588\n",
      "C21     546\n",
      "C20     525\n",
      "C12     491\n",
      "C08     473\n",
      "C01     423\n",
      "C18     388\n",
      "C17     295\n",
      "C05     283\n",
      "C13     281\n",
      "C15     215\n",
      "C16     200\n",
      "C19     191\n",
      "C11     162\n",
      "C02     158\n",
      "C09     125\n",
      "C07     100\n",
      "C22      92\n",
      "C03      65\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "train_examples_per_category = train_set['Classes'].value_counts()\n",
    "print(\"Training Set - Examples per Category:\")\n",
    "print(train_examples_per_category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Set - Examples per Category:\n",
      "Classes\n",
      "C23    2153\n",
      "C04    1467\n",
      "C14    1301\n",
      "C10     941\n",
      "C21     717\n",
      "C20     695\n",
      "C06     632\n",
      "C08     600\n",
      "C12     548\n",
      "C01     506\n",
      "C05     429\n",
      "C18     400\n",
      "C13     386\n",
      "C17     348\n",
      "C15     320\n",
      "C02     233\n",
      "C16     228\n",
      "C11     202\n",
      "C19     191\n",
      "C07     146\n",
      "C09     129\n",
      "C22      91\n",
      "C03      70\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "test_examples_per_category = test_set['Classes'].value_counts()\n",
    "print(\"\\nTest Set - Examples per Category:\")\n",
    "print(test_examples_per_category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Most Common Words:\n",
      "[('the', 85034), ('of', 84510), ('and', 57271), ('in', 55122), ('to', 30870), ('with', 30625), ('a', 30482), ('patients', 22491), ('was', 19231), ('were', 16884)]\n",
      "\n",
      "Least Common Words:\n",
      "[('dma', 1), ('suberimidate', 1), ('dms', 1), ('dithiobis', 1), ('sulfosuccinimidylpropionate', 1), ('dtssp', 1), ('opossums', 1), ('131iodine', 1), ('intraperitonealization', 1), ('ball', 1)]\n"
     ]
    }
   ],
   "source": [
    "# Calculate word frequency\n",
    "word_frequency = {}\n",
    "for word, freq in training_words:\n",
    "    word_frequency[word] = freq\n",
    "\n",
    "# Most common words\n",
    "most_common_words = training_words[:10]\n",
    "print(\"\\nMost Common Words:\")\n",
    "print(most_common_words)\n",
    "\n",
    "# Least common words\n",
    "least_common_words = training_words[-10:]\n",
    "print(\"\\nLeast Common Words:\")\n",
    "print(least_common_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Most Common Words:\n",
      "[('of', 106687), ('the', 105229), ('and', 71681), ('in', 69009), ('with', 38515), ('to', 37885), ('a', 37853), ('patients', 27426), ('was', 23847), ('were', 21782)]\n",
      "\n",
      "Least Common Words:\n",
      "[('approximates', 1), ('intratumorally', 1), ('perilesionally', 1), ('karyotypically', 1), ('foster', 1), ('greeted', 1), ('tempered', 1), ('earn', 1), ('nonmitochondrial', 1), ('saponin', 1)]\n"
     ]
    }
   ],
   "source": [
    "# Calculate word frequency\n",
    "word_frequency = {}\n",
    "for word, freq in test_words:\n",
    "    word_frequency[word] = freq\n",
    "\n",
    "# Most common words\n",
    "most_common_words = test_words[:10]\n",
    "print(\"\\nMost Common Words:\")\n",
    "print(most_common_words)\n",
    "\n",
    "# Least common words\n",
    "least_common_words = test_words[-10:]\n",
    "print(\"\\nLeast Common Words:\")\n",
    "print(least_common_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the training set, there are 10433 examples distributed across 23 unique classes. The dataset comprises 6286 unique texts. The most represented class is `C23` with a frequency of 1799 occurrences. The most frequently occurring word is `the` with a frequency of 85034. Conversely, the least frequent word is `suberimidate` occurring only once.\n",
    "\n",
    "In the test set, there are 12733 examples distributed across 23 unique classes. The dataset contains 7643 unique texts. Similar to the training set, the most represented class is `C23` with a frequency of 2153 occurrences. The most frequently occurring word is `of` with a frequency of 106687. The least frequent word is `approximates` also occurring only once."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the pre-processing phase, several crucial steps were executed to prepare the raw text data for efficient utilization in a text classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_stops = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_classes_to_integers(classes):\n",
    "    unique_classes = classes.unique()\n",
    "    class_mapping = {cls: int(cls[1:]) for cls in unique_classes}\n",
    "    return classes.replace(class_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The text underwent a series of cleaning operations. This involved the removal of HTML tags and non-alphabetic characters from the articles, ensuring that only relevant textual content remained for analysis.<br>\n",
    "Following this, two essential text refinement techniques were applied. Stopwords, common words like \"the\" or \"and\" that hold little discriminatory value, were eliminated to focus on more meaningful words. Additionally, stemming was performed using the Porter Stemmer algorithm from NLTK, reducing words to their root forms for consistency in analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(texts: defaultdict[any, list]):\n",
    "    stemmer = PorterStemmer()\n",
    "    df = get_df(texts)\n",
    "\n",
    "    x_data = df['Texts']\n",
    "    y_data = df['Classes']\n",
    "\n",
    "    # PRE-PROCESS REVIEW\n",
    "    x_data = x_data.replace({'<.*?>': ''}, regex = True)          # remove html tag\n",
    "    x_data = x_data.replace({'[^A-Za-z]': ' '}, regex = True)     # remove non alphabet\n",
    "    x_data = x_data.apply(lambda review: [w for w in review.split() if w not in english_stops])  # remove stop words\n",
    "    x_data = x_data.apply(lambda review: [w.lower() for w in review])   # lower case\n",
    "    x_data = x_data.apply(lambda review: [stemmer.stem(w) for w in review]) # perform stemming\n",
    "    \n",
    "\n",
    "    # Replace class name by their number\n",
    "    y_data = convert_classes_to_integers(y_data)\n",
    "\n",
    "    return x_data, y_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count                                                 10433\n",
       "unique                                                 6286\n",
       "top       [magnet, reson, imag, radiat, optic, neuropath...\n",
       "freq                                                      6\n",
       "Name: Texts, dtype: object"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train, y_train = load_dataset(training_texts)\n",
    "x_test, y_test = load_dataset(test_texts)\n",
    "\n",
    "x_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_length():\n",
    "    review_length = []\n",
    "    for review in x_train:\n",
    "        review_length.append(len(review))\n",
    "\n",
    "    return int(np.ceil(np.mean(review_length)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, the text underwent tokenization, a process where it was split into individual words to create sequences. These sequences were then transformed into numerical representations using encoding methods. This encoding was necessary for the text to be interpreted by the neural network model.<br>\n",
    "To maintain consistency in the input data for the neural network, sequences were padded or truncated to a fixed length using the pad_sequences function. This ensured uniformity in sequence length across all articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded X Train\n",
      " [[ 1714 10950     6 ...  4898   194   881]\n",
      " [  269  1088  4900 ...  6597    22   168]\n",
      " [  477   276   332 ...     0     0     0]\n",
      " ...\n",
      " [ 3985  3986  1553 ...  3985  3986   188]\n",
      " [   17  4568   702 ...   111     8     9]\n",
      " [  116   489   477 ...   899  7877  2850]] \n",
      "\n",
      "Encoded X Test\n",
      " [[3051 1555 1052 ... 1407  314  594]\n",
      " [ 764 8901  878 ...  161   26  537]\n",
      " [ 738   68  601 ...   18 2131  774]\n",
      " ...\n",
      " [ 537 1156 3812 ...  785   34  940]\n",
      " [ 182  208  334 ... 1481 3547  974]\n",
      " [ 472 2612 1706 ...  151   90   98]] \n",
      "\n",
      "Maximum review length:  92\n"
     ]
    }
   ],
   "source": [
    "# ENCODE REVIEW\n",
    "token = Tokenizer(lower=False)\n",
    "token.fit_on_texts(x_train)\n",
    "x_train = token.texts_to_sequences(x_train)\n",
    "x_test = token.texts_to_sequences(x_test)\n",
    "\n",
    "max_length = get_max_length() -20\n",
    "\n",
    "x_train = pad_sequences(x_train, maxlen=max_length, padding='post', truncating='post')\n",
    "x_test = pad_sequences(x_test, maxlen=max_length, padding='post', truncating='post')\n",
    "\n",
    "total_words = len(token.word_index) + 1   # add 1 because of 0 padding\n",
    "\n",
    "print('Encoded X Train\\n', x_train, '\\n')\n",
    "print('Encoded X Test\\n', x_test, '\\n')\n",
    "print('Maximum review length: ', max_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amélioration du modèle :\n",
    "- LSTM modèle avec Dense de 24 units \n",
    "- Augmentation du batch size\n",
    "- Utilisation du modèle LSTM dans les 2 sens (Bidirectional)\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first model, I utilized a Long Short-Term Memory (LSTM) network, a type of recurrent neural network (RNN) designed to process sequences of data such as text. This initial model consisted of an Embedding layer, an LSTM layer, and a Dense output layer. The total trainable parameters in this LSTM model amounted to 664632.<br>\n",
    "Upon fitting the model to the training data, the achieved accuracy was 29.17%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 92, 32)            638240    \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 64)                24832     \n",
      "                                                                 \n",
      " dense (Dense)               (None, 24)                1560      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 664,632\n",
      "Trainable params: 664,632\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# ARCHITECTURE\n",
    "EMBED_DIM = 32\n",
    "LSTM_OUT = 64\n",
    "model = Sequential()\n",
    "model.add(Embedding(total_words, EMBED_DIM, input_length=max_length))\n",
    "model.add(LSTM(LSTM_OUT))\n",
    "model.add(Dense(24, activation='softmax'))\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint(\n",
    "    'models/LSTM.h5',\n",
    "    monitor='accuracy',\n",
    "    save_best_only=True,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "81/82 [============================>.] - ETA: 0s - loss: 2.8853 - accuracy: 0.1689\n",
      "Epoch 1: accuracy improved from -inf to 0.16917, saving model to models\\LSTM.h5\n",
      "82/82 [==============================] - 7s 65ms/step - loss: 2.8852 - accuracy: 0.1692\n",
      "Epoch 2/5\n",
      "81/82 [============================>.] - ETA: 0s - loss: 2.7817 - accuracy: 0.1724\n",
      "Epoch 2: accuracy improved from 0.16917 to 0.17234, saving model to models\\LSTM.h5\n",
      "82/82 [==============================] - 5s 64ms/step - loss: 2.7820 - accuracy: 0.1723\n",
      "Epoch 3/5\n",
      "81/82 [============================>.] - ETA: 0s - loss: 2.6723 - accuracy: 0.2102\n",
      "Epoch 3: accuracy improved from 0.17234 to 0.20991, saving model to models\\LSTM.h5\n",
      "82/82 [==============================] - 5s 65ms/step - loss: 2.6722 - accuracy: 0.2099\n",
      "Epoch 4/5\n",
      "81/82 [============================>.] - ETA: 0s - loss: 2.5122 - accuracy: 0.2583\n",
      "Epoch 4: accuracy improved from 0.20991 to 0.25793, saving model to models\\LSTM.h5\n",
      "82/82 [==============================] - 5s 66ms/step - loss: 2.5136 - accuracy: 0.2579\n",
      "Epoch 5/5\n",
      "82/82 [==============================] - ETA: 0s - loss: 2.4129 - accuracy: 0.2917\n",
      "Epoch 5: accuracy improved from 0.25793 to 0.29167, saving model to models\\LSTM.h5\n",
      "82/82 [==============================] - 7s 81ms/step - loss: 2.4129 - accuracy: 0.2917\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x22882c07a60>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, batch_size = 128, epochs = 5, callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the second model, I introduced several additional layers to enhance its complexity and performance.\n",
    "\n",
    "- Embedding Layer: This layer is responsible for creating word vectors for each word in the provided word index. It groups words with similar meanings or relationships by analyzing their context within the text data. In this model, an embedding dimension of 128 is used, allowing for a higher-dimensional representation of words.\n",
    "\n",
    "- Conv1D Layer: The 1-dimensional Convolutional Neural Network layer processes the embedded word vectors by applying filters of size `128` and using the `relu` activation function. This step helps in capturing local patterns and features within the sequences of words.\n",
    "\n",
    "- MaxPooling1D Layer performs down-sampling by extracting the most important features\n",
    "\n",
    "- Bidirectional LSTM Layer: This layer incorporates Bidirectional Long Short-Term Memory units, which process input sequences in both forward and backward directions.\n",
    "\n",
    "- Dense Layer receives the processed information from the previous layers and performs computations using a `softmax` activation function with `24 units`.\n",
    "\n",
    "The optimizer used is `Adam`. Additionally, the loss function employed is `Sparse Categorical Crossentropy` as it's suitable for multi-class classification tasks with integer labels.\n",
    "<br>\n",
    "\n",
    "This extended architecture resulted in a model with a total of 2904344 trainable parameters.<br>\n",
    "Upon training this more complex model, the accuracy significantly improved to 48.56%. The added layers and increased model complexity notably contributed to the enhanced accuracy compared to the initial LSTM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, 92, 128)           2552960   \n",
      "                                                                 \n",
      " conv1d (Conv1D)             (None, 88, 128)           82048     \n",
      "                                                                 \n",
      " max_pooling1d (MaxPooling1D  (None, 22, 128)          0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " bidirectional (Bidirectiona  (None, 256)              263168    \n",
      " l)                                                              \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 24)                6168      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,904,344\n",
      "Trainable params: 2,904,344\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# ARCHITECTURE\n",
    "EMBED_DIM = 128\n",
    "LSTM_OUT = 128\n",
    "model = Sequential()\n",
    "model.add(Embedding(total_words, EMBED_DIM, input_length=max_length))\n",
    "model.add(Conv1D(128, 5, activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=4))\n",
    "model.add(Bidirectional(LSTM(LSTM_OUT, dropout=0.2)))\n",
    "model.add(Dense(24, activation='softmax'))\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint(\n",
    "    'models/LSTM.h5',\n",
    "    monitor='accuracy',\n",
    "    save_best_only=True,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82/82 [==============================] - ETA: 0s - loss: 2.7532 - accuracy: 0.1919\n",
      "Epoch 1: accuracy improved from -inf to 0.19189, saving model to models\\LSTM.h5\n",
      "82/82 [==============================] - 15s 138ms/step - loss: 2.7532 - accuracy: 0.1919\n",
      "Epoch 2/5\n",
      "82/82 [==============================] - ETA: 0s - loss: 2.3312 - accuracy: 0.3093\n",
      "Epoch 2: accuracy improved from 0.19189 to 0.30931, saving model to models\\LSTM.h5\n",
      "82/82 [==============================] - 11s 131ms/step - loss: 2.3312 - accuracy: 0.3093\n",
      "Epoch 3/5\n",
      "82/82 [==============================] - ETA: 0s - loss: 1.9911 - accuracy: 0.3868\n",
      "Epoch 3: accuracy improved from 0.30931 to 0.38675, saving model to models\\LSTM.h5\n",
      "82/82 [==============================] - 11s 129ms/step - loss: 1.9911 - accuracy: 0.3868\n",
      "Epoch 4/5\n",
      "82/82 [==============================] - ETA: 0s - loss: 1.7113 - accuracy: 0.4491\n",
      "Epoch 4: accuracy improved from 0.38675 to 0.44906, saving model to models\\LSTM.h5\n",
      "82/82 [==============================] - 11s 133ms/step - loss: 1.7113 - accuracy: 0.4491\n",
      "Epoch 5/5\n",
      "82/82 [==============================] - ETA: 0s - loss: 1.5044 - accuracy: 0.4856\n",
      "Epoch 5: accuracy improved from 0.44906 to 0.48557, saving model to models\\LSTM.h5\n",
      "82/82 [==============================] - 11s 132ms/step - loss: 1.5044 - accuracy: 0.4856\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x228f365bdc0>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, batch_size = 128, epochs = 5, callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "398/398 [==============================] - 6s 14ms/step - loss: 1.9650 - accuracy: 0.3951\n",
      "Loss: 1.9650187492370605\n",
      "Accuracy: 0.39511504769325256\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(x_test, y_test)\n",
    "print('Loss: {}'.format(loss))\n",
    "print('Accuracy: {}'.format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I evaluated the trained model using the test dataset to assess its performance on unseen data. The model's performance metrics, including loss and accuracy, were computed based on how well it predicted the categories for the articles in the test dataset.<br>\n",
    "The model achieved a test set accuracy of approximately 39.51% and a corresponding loss of approximately 1.97.<br>\n",
    "Considering the performance, the model seems to neither severely overfit nor underfit the data. <br>\n",
    "Given the accuracy and loss values obtained, it appears that the model has learned some patterns from the training data and can generalize reasonably well to the unseen test data. However, the accuracy is relatively modest, suggesting that there's room for improvement in capturing more intricate relationships within the text data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py35env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
