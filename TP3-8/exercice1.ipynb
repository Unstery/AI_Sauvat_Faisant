{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AI - TP3_8\n",
    "\n",
    "Bastien SAUVAT et Bastien FAISANT\n",
    "\n",
    "# Exercise 1 : Explorer the transformers with Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import defaultdict\n",
    "from keras.preprocessing.text import text_to_word_sequence, Tokenizer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence, Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement a Transformer block as a layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super().__init__()\n",
    "        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.ffn = keras.Sequential(\n",
    "            [\n",
    "                layers.Dense(ff_dim, activation=\"relu\"),\n",
    "                layers.Dense(embed_dim),\n",
    "            ]\n",
    "        )\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement embedding layer\n",
    "\n",
    "Two seperate embedding layers, one for tokens, one for token index (positions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenAndPositionEmbedding(layers.Layer):\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
    "        super().__init__()\n",
    "        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
    "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        maxlen = tf.shape(x)[-1]\n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "        positions = self.pos_emb(positions)\n",
    "        x = self.token_emb(x)\n",
    "        return x + positions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_info(path: str):\n",
    "    data = list(os.walk(path))[1:]\n",
    "    files = []\n",
    "    for d in data:\n",
    "        folder_name = d[0]\n",
    "        for file in d[2]:\n",
    "            files.append((folder_name.split('/')[-1], os.path.join(folder_name, file)))\n",
    "\n",
    "    d = defaultdict(int)\n",
    "    texts = defaultdict(list)\n",
    "    for (cate, file) in files:\n",
    "        with open(file, 'r') as outfile:\n",
    "            text = outfile.read()\n",
    "            texts[cate].append(text)\n",
    "            words = text_to_word_sequence(text)\n",
    "            for word in words:\n",
    "                d[word] += 1\n",
    "    words = sorted(d.items(), key=lambda x: x[1], reverse=True)\n",
    "    return (texts, words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_texts, training_words = get_info(\"../TP1-2/data/ohsumed-first-20000-docs/training/\")\n",
    "test_texts, test_words = get_info(\"../TP1-2/data/ohsumed-first-20000-docs/test/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df(dataset: defaultdict[any, list]):\n",
    "    classes = []\n",
    "    texts = []\n",
    "    for classe, liste_texts in dataset.items():\n",
    "        for text in liste_texts:\n",
    "            texts.append(text)\n",
    "            classes.append(classe)\n",
    "\n",
    "    df = pd.DataFrame({'Classes': classes, 'Texts': texts})\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = get_df(training_texts)\n",
    "test_set = get_df(test_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_stops = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_classes_to_integers(classes):\n",
    "    unique_classes = classes.unique()\n",
    "    class_mapping = {cls: int(cls[1:]) for cls in unique_classes}\n",
    "    return classes.replace(class_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(texts: defaultdict[any, list]):\n",
    "    stemmer = PorterStemmer()\n",
    "    df = get_df(texts)\n",
    "\n",
    "    x_data = df['Texts']\n",
    "    y_data = df['Classes']\n",
    "\n",
    "    # PRE-PROCESS REVIEW\n",
    "    x_data = x_data.replace({'<.*?>': ''}, regex = True)          # remove html tag\n",
    "    x_data = x_data.replace({'[^A-Za-z]': ' '}, regex = True)     # remove non alphabet\n",
    "    x_data = x_data.apply(lambda review: [w for w in review.split() if w not in english_stops])  # remove stop words\n",
    "    x_data = x_data.apply(lambda review: [w.lower() for w in review])   # lower case\n",
    "    x_data = x_data.apply(lambda review: [stemmer.stem(w) for w in review]) # perform stemming\n",
    "    \n",
    "\n",
    "    # Replace class name by their number\n",
    "    y_data = convert_classes_to_integers(y_data)\n",
    "\n",
    "    return x_data, y_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = load_dataset(training_texts)\n",
    "x_test, y_test = load_dataset(test_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_length():\n",
    "    review_length = []\n",
    "    for review in x_train:\n",
    "        review_length.append(len(review))\n",
    "\n",
    "    return int(np.ceil(np.mean(review_length)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ENCODE REVIEW\n",
    "token = Tokenizer(lower=False)\n",
    "token.fit_on_texts(x_train)\n",
    "x_train = token.texts_to_sequences(x_train)\n",
    "x_test = token.texts_to_sequences(x_test)\n",
    "\n",
    "max_length = get_max_length()\n",
    "\n",
    "x_train = pad_sequences(x_train, maxlen=max_length, padding='post', truncating='post')\n",
    "x_test = pad_sequences(x_test, maxlen=max_length, padding='post', truncating='post')\n",
    "\n",
    "total_words = len(token.word_index) + 1   # add 1 because of 0 padding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create classifier model using transformer layer\n",
    "\n",
    "Transformer layer outputs one vector for each time step of our input sequence.\n",
    "Here, we take the mean across all time steps and\n",
    "use a feed forward network on top of it to classify text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_20\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_22 (InputLayer)       [(None, 112)]             0         \n",
      "                                                                 \n",
      " token_and_position_embeddin  (None, 112, 32)          641824    \n",
      " g_21 (TokenAndPositionEmbed                                     \n",
      " ding)                                                           \n",
      "                                                                 \n",
      " transformer_block_21 (Trans  (None, 112, 32)          10656     \n",
      " formerBlock)                                                    \n",
      "                                                                 \n",
      " global_average_pooling1d_20  (None, 32)               0         \n",
      "  (GlobalAveragePooling1D)                                       \n",
      "                                                                 \n",
      " dropout_64 (Dropout)        (None, 32)                0         \n",
      "                                                                 \n",
      " dense_64 (Dense)            (None, 24)                792       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 653,272\n",
      "Trainable params: 653,272\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "embed_dim = 32  # Embedding size for each token\n",
    "num_heads = 2  # Number of attention heads\n",
    "ff_dim = 32  # Hidden layer size in feed forward network inside transformer\n",
    "\n",
    "inputs = layers.Input(shape=(max_length,))\n",
    "embedding_layer = TokenAndPositionEmbedding(max_length, total_words, embed_dim)\n",
    "x = embedding_layer(inputs)\n",
    "transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)\n",
    "x = transformer_block(x)\n",
    "x = layers.GlobalAveragePooling1D()(x)\n",
    "x = layers.Dropout(0.2)(x)\n",
    "outputs = layers.Dense(24, activation=\"softmax\")(x)\n",
    "\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "164/164 [==============================] - 18s 103ms/step - loss: 2.5934 - accuracy: 0.2355\n",
      "Epoch 2/5\n",
      "164/164 [==============================] - 16s 100ms/step - loss: 1.9289 - accuracy: 0.3967\n",
      "Epoch 3/5\n",
      "164/164 [==============================] - 17s 107ms/step - loss: 1.5397 - accuracy: 0.4731\n",
      "Epoch 4/5\n",
      "164/164 [==============================] - 17s 106ms/step - loss: 1.3319 - accuracy: 0.4921\n",
      "Epoch 5/5\n",
      "164/164 [==============================] - 17s 104ms/step - loss: 1.2060 - accuracy: 0.5059\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    x_train, y_train, batch_size=64, epochs=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "398/398 [==============================] - 8s 19ms/step - loss: 1.9134 - accuracy: 0.4031\n",
      "Loss: 1.9133960008621216\n",
      "Accuracy: 0.40312573313713074\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(x_test, y_test)\n",
    "print('Loss: {}'.format(loss))\n",
    "print('Accuracy: {}'.format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When comparing the Transformer-based model and the LSTM-based classifier, no significant improvement in accuracy was obtained, compared to different configurations of the Transformer model applied to the Ohsumed dataset. similar whether for the training set (between 0.44 and 0.51) and the test set (between 0.38 and 0.42)<br>\n",
    "\n",
    "Upon systematic analysis of the impact of hyperparameter changes on the Transformer model, various adjustments like altering batch size, the number of attention heads, the embedding size and the dropout displayed marginal variations in performance.<br>\n",
    "For instance, modifying batch sizes did not significantly affect the model's accuracy, which remained approximately around 0.42 for the test set.<br>\n",
    "Additionally, manipulating the number of attention heads between 1 and 10 did not lead to substantial improvements in accuracy, staying within the range of 0.38 to 0.42. However, a negative point is that increasing the number of attentionheads increases the number of trainable parameters of the model and therefore the training time.<br>\n",
    "Furthermore, if we increase the embedding size from 32 to 64, the number of trainable parameters increases drastically and therefore directly influences the training time of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transformer-based model performance\n",
    "- With batch_size = 32 :\n",
    "  - training set accuracy=0.47\n",
    "  - test set accuracy=0.42\n",
    "- With batch_size = 128\n",
    "  - training set accuracy=0.48\n",
    "  - test set accuracy=0.42\n",
    "- Decrease attention heads to 1 :\n",
    "  - training set accuracy=0.44\n",
    "  - test set accuracy=0.38\n",
    "- Increase attention heads Ã  3 :\n",
    "  - training set accuracy=0.44\n",
    "  - test set accuracy=0.39\n",
    "- Increase attention heads to 4 :\n",
    "  - training set accuracy=0.45\n",
    "  - test set accuracy=0.38\n",
    "- Increase attention heads to 10 :\n",
    "  - training set accuracy=0.47\n",
    "  - test set accuracy=0.39\n",
    "- Increase dropout to 0.2 :\n",
    "  - training set accuracy=0.50\n",
    "  - test set accuracy=0.41\n",
    "- Increase embedding size to 64 :\n",
    "  - training set accuracy=0.50\n",
    "  - test set accuracy=0.40\n",
    "- Increase dropout to 0.4 :\n",
    "  - training set accuracy=0.49\n",
    "  - test set accuracy=0.40"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py35env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
